{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1\n",
        "Build a auto-corrector based on a (small) vocabulary of known word extracted from a text\n",
        "repository of your choice that takes as input a sentence (with possible misspelled words) and\n",
        "replaces each of the words with one from the vocabulary that minimizes the edit distance.\n",
        "Please cite your sources, show your code, and include some input-output examples.\n",
        "Discuss what other techniques (that we have discussed in previous sessions or that you can think\n",
        "of otherwise) could be used to make this simple auto-correct perform better in terms of inferring\n",
        "the intended meaning of the word or to take into account similarities in pronunciation between\n",
        "differently-spelled words."
      ],
      "metadata": {
        "id": "EptZ9oUjC9GD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### source: https://medium.com/swlh/how-to-build-an-autocorrect-in-python-7545338a1320\n",
        "#### Title: How to build an autocorrect in python \n",
        "#### Subtitle: Example of how to build an Autocorrect in Python by taking the vocabulary from a corpus\n",
        "#### Author: George Pipis\n"
      ],
      "metadata": {
        "id": "O92uVPbmCpAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EuI_xsUdBDc",
        "outputId": "09c65944-ef53-47d8-c1a9-be4121248531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textdistance\n",
            "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ETlWnLentan",
        "outputId": "7010448d-d032-4395-9ff0-4510e5a7148a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 words in our dictionary are: \n",
            "['What', 'if', 'some', 'day', 'or', 'night', 'a', 'demon', 'were', 'to', 'steal', 'after', 'you', 'into', 'your', 'loneliest', 'loneliness', 'and', 'say', 'to']\n",
            "The dictionary has 60 words \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textdistance\n",
        "import re\n",
        "from collections import Counter\n",
        "words = []\n",
        "text = '''What, if some day or night a demon were to steal after you into your loneliest loneliness and say to you:\n",
        "'This life as you now live it and have lived it, you will have to live once more and innumerable times more' ... \n",
        "Would you not throw yourself down and gnash your teeth and curse the demon who spoke thus? Or have you once experienced a\n",
        "tremendous moment when you would have answered him: 'You are a god and never have I heard anything more divine.'''\n",
        "# file_name_data = text.read()\n",
        "file_name_data=text.lower()\n",
        "words = re.findall(r'\\w+',text)\n",
        "# This is our vocabulary\n",
        "V = set(words)\n",
        "print(f\"The first 10 words in our dictionary are: \\n{words[0:20]}\")\n",
        "print(f\"The dictionary has {len(V)} words \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_count(words):\n",
        "    word_count_dict = {}\n",
        "    for word in words:\n",
        "        if word in word_count_dict:\n",
        "            word_count_dict[word] += 1\n",
        "        else:\n",
        "            word_count_dict[word] = 1\n",
        "    return word_count_dict\n",
        "word_count_dict = get_count(words)\n",
        "print(f\"There are {len(word_count_dict)} key values pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WMCNjMCqLoV",
        "outputId": "10a3c419-b9b7-4e31-edb9-a528f2d5e125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 60 key values pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq_dict = {}  \n",
        "word_freq_dict = Counter(words)\n",
        "print(word_freq_dict.most_common()[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH1rfn_D_Qnr",
        "outputId": "8f47471d-ab2f-4562-da53-c447cc4dd6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('you', 7), ('and', 6), ('have', 5), ('a', 3), ('to', 3), ('more', 3), ('demon', 2), ('your', 2), ('live', 2), ('it', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_autocorrect(input_word):\n",
        "  input_word = input_word.lower()\n",
        "  if input_word in V:\n",
        "    return('ü•≥ Wunderbar! You are a true spelling bee champion üêù!')\n",
        "  else:\n",
        "    similarities = [1-(textdistance.Levenshtein(qval=2).distance(v,input_word)) for v in word_freq_dict.keys()]\n",
        "    df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n",
        "    df = df.rename(columns={'index':'Word', 0:'Prob'})\n",
        "    df['Similarity'] = similarities\n",
        "    output = df.sort_values(['Similarity', 'Prob'], ascending=False).head(1)\n",
        "    return(output)"
      ],
      "metadata": {
        "id": "pR_idXNR_XV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_autocorrect('it')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1keskKLk_t6M",
        "outputId": "262e4f40-ccc8-4c41-c651-d830d0cf8bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ü•≥ Wunderbar! You are a true spelling bee champion üêù!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = {} \n",
        "    \n",
        "Total = sum(word_freq_dict.values())\n",
        "    \n",
        "for k in word_freq_dict.keys():\n",
        "    probs[k] = word_freq_dict[k]/Total"
      ],
      "metadata": {
        "id": "LDWX8jB4dePH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_autocorrect('F√©mons')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "yqeGE40uCVVt",
        "outputId": "69d27c0b-00ae-4d67-a9d3-87ce455e0020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Word      Prob  Similarity\n",
              "7  demon  0.023256          -2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a2e298f-046b-4e46-a045-a45a8bbf42a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Prob</th>\n",
              "      <th>Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>demon</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a2e298f-046b-4e46-a045-a45a8bbf42a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a2e298f-046b-4e46-a045-a45a8bbf42a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a2e298f-046b-4e46-a045-a45a8bbf42a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "Using either some n-gram based or another type of approach (remember to cite any sources you\n",
        "consult) and a text repository of your choice, implement a simple auto-complete that suggests\n",
        "possible options for what the next word could be, given a start of a sentence as input.\n",
        "Please include code snippets and examples, as usual.\n",
        "Discuss how the value for n affects the quality you observe (subjective or measured). Would\n",
        "you actually need a range of values for n instead of a single value for this to work well?"
      ],
      "metadata": {
        "id": "1ZKgPqAND8xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Source: https://towardsdatascience.com/exploring-the-next-word-predictor-5e22aeb85d8f\n",
        "#### Title: Exploring the next word predictor!\n",
        "#### Subtitle: Different approaches for duilding the next word predictor\n",
        "#### Author: Dhruvil Shah"
      ],
      "metadata": {
        "id": "eEWR3BWDGQ4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "ESJI2R3SR9kq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
        "lines = []\n",
        "\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "    \n",
        "print(\"The First Line: \", lines[0])\n",
        "print(\"The Last Line: \", lines[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCXtVIloSBLo",
        "outputId": "88c2dd4c-7c24-452e-929a-640f6f54762c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The First Line:  ÔªøOne morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "\n",
            "The Last Line:  first to get up and stretch out her young body.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\n",
        "\n",
        "for i in lines:\n",
        "    data = ' '. join(lines)\n",
        "    \n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "data[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Vx9sznjSQ5gz",
        "outputId": "6f880e6b-7238-42fd-966a-3242a3007ce5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.  \"What\\'s happened to me?\" he'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = []\n",
        "\n",
        "for i in data.split():\n",
        "    if i not in z:\n",
        "        z.append(i)\n",
        "        \n",
        "data = ' '.join(z)\n",
        "data[:552]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "YTBqEIQdZZ5b",
        "outputId": "1ba6e955-77bc-4650-8ab4-1f60ba357a93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room although too small, peacefully between its four familiar '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "tokens = word_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnk2m1QV-Ush",
        "outputId": "99135e42-5cef-4401-e038-4c257f5ce320"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = 3\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(tokens)):\n",
        "    seq = tokens[i-train_len:i]\n",
        "    text_sequences.append(seq)\n",
        "sequences = {}\n",
        "count = 1\n",
        "for i in range(len(tokens)):\n",
        "    if tokens[i] not in sequences:\n",
        "        sequences[tokens[i]] = count\n",
        "        count += 1\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences) "
      ],
      "metadata": {
        "id": "KhJxnkNh-EbG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collecting some information   \n",
        "vocabulary_size = len(tokenizer.word_counts)+1\n",
        "\n",
        "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
        "for i in range(len(sequences)):\n",
        "    n_sequences[i] = sequences[i]"
      ],
      "metadata": {
        "id": "4In2WTO1-JGg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = n_sequences[:,:-1]\n",
        "train_targets = n_sequences[:,-1]\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
        "seq_len = train_inputs.shape[1]\n",
        "train_inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLQszb_Z-vE9",
        "outputId": "f0a547a7-9ba1-45f0-94fa-aa3023407f82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5532, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)"
      ],
      "metadata": {
        "id": "NGAAjuXkAcCZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "#model = load_model(\"mymodel.h5\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
        "model.add(LSTM(1000,return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000,activation='relu'))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoaG1wqR-44I",
        "outputId": "68a08218-7683-453d-bd81-b8d6a2b573d1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 2, 2)              5170      \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 2, 1000)           4012000   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2585)              2587585   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,609,755\n",
            "Trainable params: 15,609,755\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "eHQcmbJRBXYP",
        "outputId": "fb6e7296-a93f-4675-d541-f18ca06a9a96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "model.fit(train_inputs,train_targets,epochs=150, batch_size=64, callbacks=[checkpoint, reduce],verbose=1)\n",
        "model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RRUuLW2BTgz",
        "outputId": "d1f845a1-fdf2-42e3-e81e-5f01d7b7fc78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 6.1921 - accuracy: 0.1235\n",
            "Epoch 1: loss improved from 6.26565 to 6.19378, saving model to nextword1.h5\n",
            "87/87 [==============================] - 5s 18ms/step - loss: 6.1938 - accuracy: 0.1235 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 5.8759 - accuracy: 0.1244\n",
            "Epoch 2: loss improved from 6.19378 to 5.87591, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 5.8759 - accuracy: 0.1244 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 5.6252 - accuracy: 0.1266\n",
            "Epoch 3: loss improved from 5.87591 to 5.62152, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 5.6215 - accuracy: 0.1269 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 5.3968 - accuracy: 0.1293\n",
            "Epoch 4: loss improved from 5.62152 to 5.40559, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 5.4056 - accuracy: 0.1296 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 5.1942 - accuracy: 0.1419\n",
            "Epoch 5: loss improved from 5.40559 to 5.19883, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 5.1988 - accuracy: 0.1421 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 5.0022 - accuracy: 0.1484\n",
            "Epoch 6: loss improved from 5.19883 to 5.00215, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 5.0022 - accuracy: 0.1484 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 4.8416 - accuracy: 0.1584\n",
            "Epoch 7: loss improved from 5.00215 to 4.84161, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 4.8416 - accuracy: 0.1584 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 4.6632 - accuracy: 0.1713\n",
            "Epoch 8: loss improved from 4.84161 to 4.67944, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 4.6794 - accuracy: 0.1701 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 4.5116 - accuracy: 0.1721\n",
            "Epoch 9: loss improved from 4.67944 to 4.51302, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 4.5130 - accuracy: 0.1732 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 4.3701 - accuracy: 0.1842\n",
            "Epoch 10: loss improved from 4.51302 to 4.37011, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 4.3701 - accuracy: 0.1842 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 4.2208 - accuracy: 0.1860\n",
            "Epoch 11: loss improved from 4.37011 to 4.22372, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 4.2237 - accuracy: 0.1856 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 4.0561 - accuracy: 0.1916\n",
            "Epoch 12: loss improved from 4.22372 to 4.05928, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 4.0593 - accuracy: 0.1920 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 3.9091 - accuracy: 0.1935\n",
            "Epoch 13: loss improved from 4.05928 to 3.90751, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 3.9075 - accuracy: 0.1941 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 3.8226 - accuracy: 0.1987\n",
            "Epoch 14: loss improved from 3.90751 to 3.81703, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 3.8170 - accuracy: 0.1996 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 3.6830 - accuracy: 0.2026\n",
            "Epoch 15: loss improved from 3.81703 to 3.68560, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 3.6856 - accuracy: 0.2021 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 3.6056 - accuracy: 0.2022\n",
            "Epoch 16: loss improved from 3.68560 to 3.60631, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 3.6063 - accuracy: 0.2025 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 3.4655 - accuracy: 0.2160\n",
            "Epoch 17: loss improved from 3.60631 to 3.47708, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 3.4771 - accuracy: 0.2140 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 3.3472 - accuracy: 0.2218\n",
            "Epoch 18: loss improved from 3.47708 to 3.36754, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 3.3675 - accuracy: 0.2191 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 3.2696 - accuracy: 0.2337\n",
            "Epoch 19: loss improved from 3.36754 to 3.26960, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 3.2696 - accuracy: 0.2337 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 3.1597 - accuracy: 0.2378\n",
            "Epoch 20: loss improved from 3.26960 to 3.17107, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 3.1711 - accuracy: 0.2366 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 3.0757 - accuracy: 0.2432\n",
            "Epoch 21: loss improved from 3.17107 to 3.08167, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 3.0817 - accuracy: 0.2424 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 3.0044 - accuracy: 0.2545\n",
            "Epoch 22: loss improved from 3.08167 to 3.01224, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 3.0122 - accuracy: 0.2529 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 2.9111 - accuracy: 0.2641\n",
            "Epoch 23: loss improved from 3.01224 to 2.91286, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.9129 - accuracy: 0.2636 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.8404 - accuracy: 0.2750\n",
            "Epoch 24: loss improved from 2.91286 to 2.84840, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.8484 - accuracy: 0.2731 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 2.7902 - accuracy: 0.2779\n",
            "Epoch 25: loss improved from 2.84840 to 2.79212, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 2.7921 - accuracy: 0.2782 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.7134 - accuracy: 0.2928\n",
            "Epoch 26: loss improved from 2.79212 to 2.71995, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.7199 - accuracy: 0.2914 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 2.6604 - accuracy: 0.3044\n",
            "Epoch 27: loss improved from 2.71995 to 2.66044, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 22ms/step - loss: 2.6604 - accuracy: 0.3044 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.5862 - accuracy: 0.3094\n",
            "Epoch 28: loss improved from 2.66044 to 2.58986, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 2.5899 - accuracy: 0.3084 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.5127 - accuracy: 0.3200\n",
            "Epoch 29: loss improved from 2.58986 to 2.51870, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.5187 - accuracy: 0.3191 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.4796 - accuracy: 0.3233\n",
            "Epoch 30: loss improved from 2.51870 to 2.48754, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.4875 - accuracy: 0.3210 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 2.3958 - accuracy: 0.3394\n",
            "Epoch 31: loss improved from 2.48754 to 2.39998, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 2.4000 - accuracy: 0.3386 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 2.3966 - accuracy: 0.3408\n",
            "Epoch 32: loss improved from 2.39998 to 2.39767, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.3977 - accuracy: 0.3400 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 2.3611 - accuracy: 0.3496\n",
            "Epoch 33: loss improved from 2.39767 to 2.36112, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 2.3611 - accuracy: 0.3496 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.3288 - accuracy: 0.3515\n",
            "Epoch 34: loss improved from 2.36112 to 2.33126, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.3313 - accuracy: 0.3507 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 2.2294 - accuracy: 0.3694\n",
            "Epoch 35: loss improved from 2.33126 to 2.23458, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 2.2346 - accuracy: 0.3689 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 2.1509 - accuracy: 0.3899\n",
            "Epoch 36: loss improved from 2.23458 to 2.15548, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.1555 - accuracy: 0.3885 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 2.1348 - accuracy: 0.3987\n",
            "Epoch 37: loss improved from 2.15548 to 2.14622, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 2.1462 - accuracy: 0.3944 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 2.1449 - accuracy: 0.3871\n",
            "Epoch 38: loss did not improve from 2.14622\n",
            "87/87 [==============================] - 1s 12ms/step - loss: 2.1513 - accuracy: 0.3859 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 2.0779 - accuracy: 0.3999\n",
            "Epoch 39: loss improved from 2.14622 to 2.08223, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 2.0822 - accuracy: 0.3979 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 2.0187 - accuracy: 0.4160\n",
            "Epoch 40: loss improved from 2.08223 to 2.02586, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 2.0259 - accuracy: 0.4154 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 1.9717 - accuracy: 0.4202\n",
            "Epoch 41: loss improved from 2.02586 to 1.98149, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.9815 - accuracy: 0.4192 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 1.9815 - accuracy: 0.4235\n",
            "Epoch 42: loss did not improve from 1.98149\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 1.9815 - accuracy: 0.4235 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 1.9679 - accuracy: 0.4290\n",
            "Epoch 43: loss improved from 1.98149 to 1.97028, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.9703 - accuracy: 0.4279 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 1.9366 - accuracy: 0.4272\n",
            "Epoch 44: loss improved from 1.97028 to 1.93661, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.9366 - accuracy: 0.4272 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.9166 - accuracy: 0.4446\n",
            "Epoch 45: loss improved from 1.93661 to 1.91398, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.9140 - accuracy: 0.4452 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 1.9009 - accuracy: 0.4358\n",
            "Epoch 46: loss improved from 1.91398 to 1.90561, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.9056 - accuracy: 0.4342 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.8700 - accuracy: 0.4449\n",
            "Epoch 47: loss improved from 1.90561 to 1.87187, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.8719 - accuracy: 0.4447 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 1.8057 - accuracy: 0.4605\n",
            "Epoch 48: loss improved from 1.87187 to 1.80643, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.8064 - accuracy: 0.4608 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 1.7437 - accuracy: 0.4784\n",
            "Epoch 49: loss improved from 1.80643 to 1.75088, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.7509 - accuracy: 0.4758 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.7355 - accuracy: 0.4777\n",
            "Epoch 50: loss improved from 1.75088 to 1.73767, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.7377 - accuracy: 0.4769 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 1.7066 - accuracy: 0.4796\n",
            "Epoch 51: loss improved from 1.73767 to 1.70710, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.7071 - accuracy: 0.4794 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 1.6691 - accuracy: 0.4923\n",
            "Epoch 52: loss improved from 1.70710 to 1.68020, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.6802 - accuracy: 0.4888 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 1.6573 - accuracy: 0.4924\n",
            "Epoch 53: loss improved from 1.68020 to 1.66773, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.6677 - accuracy: 0.4904 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.6872 - accuracy: 0.4835\n",
            "Epoch 54: loss did not improve from 1.66773\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 1.6894 - accuracy: 0.4830 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 1.6613 - accuracy: 0.4980\n",
            "Epoch 55: loss improved from 1.66773 to 1.66200, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.6620 - accuracy: 0.4978 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 1.6633 - accuracy: 0.4887\n",
            "Epoch 56: loss did not improve from 1.66200\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 1.6706 - accuracy: 0.4872 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 1.6322 - accuracy: 0.4983\n",
            "Epoch 57: loss improved from 1.66200 to 1.63309, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.6331 - accuracy: 0.4991 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 1.6138 - accuracy: 0.4977\n",
            "Epoch 58: loss improved from 1.63309 to 1.62768, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.6277 - accuracy: 0.4949 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 1.6215 - accuracy: 0.5051\n",
            "Epoch 59: loss improved from 1.62768 to 1.62146, saving model to nextword1.h5\n",
            "87/87 [==============================] - 1s 17ms/step - loss: 1.6215 - accuracy: 0.5051 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.5654 - accuracy: 0.5091\n",
            "Epoch 60: loss improved from 1.62146 to 1.56503, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.5650 - accuracy: 0.5092 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.5180 - accuracy: 0.5332\n",
            "Epoch 61: loss improved from 1.56503 to 1.51799, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.5180 - accuracy: 0.5329 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 1.5158 - accuracy: 0.5268\n",
            "Epoch 62: loss improved from 1.51799 to 1.51576, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.5158 - accuracy: 0.5268 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 1.5069 - accuracy: 0.5375\n",
            "Epoch 63: loss improved from 1.51576 to 1.51139, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.5114 - accuracy: 0.5351 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.4866 - accuracy: 0.5407\n",
            "Epoch 64: loss improved from 1.51139 to 1.48753, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.4875 - accuracy: 0.5401 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 1.4805 - accuracy: 0.5365\n",
            "Epoch 65: loss did not improve from 1.48753\n",
            "87/87 [==============================] - 1s 12ms/step - loss: 1.4876 - accuracy: 0.5349 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.4776 - accuracy: 0.5394\n",
            "Epoch 66: loss improved from 1.48753 to 1.47701, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.4770 - accuracy: 0.5398 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 1.4348 - accuracy: 0.5497\n",
            "Epoch 67: loss improved from 1.47701 to 1.43593, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.4359 - accuracy: 0.5495 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.4379 - accuracy: 0.5501\n",
            "Epoch 68: loss did not improve from 1.43593\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 1.4409 - accuracy: 0.5499 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 1.4262 - accuracy: 0.5456\n",
            "Epoch 69: loss improved from 1.43593 to 1.42886, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 1.4289 - accuracy: 0.5450 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 1.4352 - accuracy: 0.5465\n",
            "Epoch 70: loss did not improve from 1.42886\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 1.4352 - accuracy: 0.5465 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 1.4087 - accuracy: 0.5514\n",
            "Epoch 71: loss did not improve from 1.42886\n",
            "87/87 [==============================] - 1s 12ms/step - loss: 1.4293 - accuracy: 0.5446 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.4500 - accuracy: 0.5431\n",
            "Epoch 72: loss did not improve from 1.42886\n",
            "\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 1.4489 - accuracy: 0.5434 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 1.1383 - accuracy: 0.6414\n",
            "Epoch 73: loss improved from 1.42886 to 1.13707, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 1.1371 - accuracy: 0.6414 - lr: 2.0000e-04\n",
            "Epoch 74/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.9954 - accuracy: 0.6977\n",
            "Epoch 74: loss improved from 1.13707 to 0.99559, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.9956 - accuracy: 0.6970 - lr: 2.0000e-04\n",
            "Epoch 75/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.9492 - accuracy: 0.7200\n",
            "Epoch 75: loss improved from 0.99559 to 0.94728, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.9473 - accuracy: 0.7205 - lr: 2.0000e-04\n",
            "Epoch 76/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.9204 - accuracy: 0.7334\n",
            "Epoch 76: loss improved from 0.94728 to 0.91929, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.9193 - accuracy: 0.7336 - lr: 2.0000e-04\n",
            "Epoch 77/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.9009 - accuracy: 0.7361\n",
            "Epoch 77: loss improved from 0.91929 to 0.89978, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.8998 - accuracy: 0.7361 - lr: 2.0000e-04\n",
            "Epoch 78/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.8849 - accuracy: 0.7414\n",
            "Epoch 78: loss improved from 0.89978 to 0.88443, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.8844 - accuracy: 0.7417 - lr: 2.0000e-04\n",
            "Epoch 79/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.8761 - accuracy: 0.7397\n",
            "Epoch 79: loss improved from 0.88443 to 0.87304, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.8730 - accuracy: 0.7410 - lr: 2.0000e-04\n",
            "Epoch 80/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.8580 - accuracy: 0.7483\n",
            "Epoch 80: loss improved from 0.87304 to 0.85858, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.8586 - accuracy: 0.7482 - lr: 2.0000e-04\n",
            "Epoch 81/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.8499 - accuracy: 0.7476\n",
            "Epoch 81: loss improved from 0.85858 to 0.84949, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.8495 - accuracy: 0.7475 - lr: 2.0000e-04\n",
            "Epoch 82/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.8395 - accuracy: 0.7482\n",
            "Epoch 82: loss improved from 0.84949 to 0.84314, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.8431 - accuracy: 0.7460 - lr: 2.0000e-04\n",
            "Epoch 83/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.8357 - accuracy: 0.7515\n",
            "Epoch 83: loss improved from 0.84314 to 0.83472, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 21ms/step - loss: 0.8347 - accuracy: 0.7514 - lr: 2.0000e-04\n",
            "Epoch 84/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.8202 - accuracy: 0.7489\n",
            "Epoch 84: loss improved from 0.83472 to 0.82353, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.8235 - accuracy: 0.7473 - lr: 2.0000e-04\n",
            "Epoch 85/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.8185 - accuracy: 0.7504\n",
            "Epoch 85: loss improved from 0.82353 to 0.81852, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.8185 - accuracy: 0.7504 - lr: 2.0000e-04\n",
            "Epoch 86/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.8045 - accuracy: 0.7522\n",
            "Epoch 86: loss improved from 0.81852 to 0.80713, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.8071 - accuracy: 0.7513 - lr: 2.0000e-04\n",
            "Epoch 87/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.7944 - accuracy: 0.7597\n",
            "Epoch 87: loss improved from 0.80713 to 0.79849, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7985 - accuracy: 0.7565 - lr: 2.0000e-04\n",
            "Epoch 88/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.7927 - accuracy: 0.7525\n",
            "Epoch 88: loss improved from 0.79849 to 0.79369, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7937 - accuracy: 0.7525 - lr: 2.0000e-04\n",
            "Epoch 89/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.7860 - accuracy: 0.7588\n",
            "Epoch 89: loss improved from 0.79369 to 0.78702, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.7870 - accuracy: 0.7572 - lr: 2.0000e-04\n",
            "Epoch 90/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.7853 - accuracy: 0.7540\n",
            "Epoch 90: loss improved from 0.78702 to 0.78622, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 29ms/step - loss: 0.7862 - accuracy: 0.7534 - lr: 2.0000e-04\n",
            "Epoch 91/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.7692 - accuracy: 0.7592\n",
            "Epoch 91: loss improved from 0.78622 to 0.77383, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.7738 - accuracy: 0.7574 - lr: 2.0000e-04\n",
            "Epoch 92/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.7688 - accuracy: 0.7612\n",
            "Epoch 92: loss improved from 0.77383 to 0.76884, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7688 - accuracy: 0.7612 - lr: 2.0000e-04\n",
            "Epoch 93/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.7689 - accuracy: 0.7551\n",
            "Epoch 93: loss did not improve from 0.76884\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 0.7712 - accuracy: 0.7542 - lr: 2.0000e-04\n",
            "Epoch 94/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.7620 - accuracy: 0.7627\n",
            "Epoch 94: loss improved from 0.76884 to 0.76198, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7620 - accuracy: 0.7627 - lr: 2.0000e-04\n",
            "Epoch 95/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.7505 - accuracy: 0.7650\n",
            "Epoch 95: loss improved from 0.76198 to 0.75047, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.7505 - accuracy: 0.7650 - lr: 2.0000e-04\n",
            "Epoch 96/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.7448 - accuracy: 0.7682\n",
            "Epoch 96: loss improved from 0.75047 to 0.74521, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7452 - accuracy: 0.7679 - lr: 2.0000e-04\n",
            "Epoch 97/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.7421 - accuracy: 0.7679\n",
            "Epoch 97: loss improved from 0.74521 to 0.74209, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7421 - accuracy: 0.7679 - lr: 2.0000e-04\n",
            "Epoch 98/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.7323 - accuracy: 0.7647\n",
            "Epoch 98: loss improved from 0.74209 to 0.73750, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7375 - accuracy: 0.7630 - lr: 2.0000e-04\n",
            "Epoch 99/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.7298 - accuracy: 0.7683\n",
            "Epoch 99: loss improved from 0.73750 to 0.73155, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7316 - accuracy: 0.7666 - lr: 2.0000e-04\n",
            "Epoch 100/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.7213 - accuracy: 0.7708\n",
            "Epoch 100: loss improved from 0.73155 to 0.72685, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7268 - accuracy: 0.7690 - lr: 2.0000e-04\n",
            "Epoch 101/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.7260 - accuracy: 0.7641\n",
            "Epoch 101: loss improved from 0.72685 to 0.72600, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7260 - accuracy: 0.7641 - lr: 2.0000e-04\n",
            "Epoch 102/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.7113 - accuracy: 0.7767\n",
            "Epoch 102: loss improved from 0.72600 to 0.71435, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7143 - accuracy: 0.7748 - lr: 2.0000e-04\n",
            "Epoch 103/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.7085 - accuracy: 0.7773\n",
            "Epoch 103: loss improved from 0.71435 to 0.70881, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7088 - accuracy: 0.7773 - lr: 2.0000e-04\n",
            "Epoch 104/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.7069 - accuracy: 0.7757\n",
            "Epoch 104: loss improved from 0.70881 to 0.70694, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7069 - accuracy: 0.7757 - lr: 2.0000e-04\n",
            "Epoch 105/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.6997 - accuracy: 0.7768\n",
            "Epoch 105: loss improved from 0.70694 to 0.70247, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.7025 - accuracy: 0.7744 - lr: 2.0000e-04\n",
            "Epoch 106/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.7818\n",
            "Epoch 106: loss improved from 0.70247 to 0.69584, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6958 - accuracy: 0.7818 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.7823\n",
            "Epoch 107: loss improved from 0.69584 to 0.69457, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6946 - accuracy: 0.7820 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.7789\n",
            "Epoch 108: loss improved from 0.69457 to 0.69134, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6913 - accuracy: 0.7789 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.6859 - accuracy: 0.7837\n",
            "Epoch 109: loss improved from 0.69134 to 0.68721, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6872 - accuracy: 0.7831 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.6727 - accuracy: 0.7877\n",
            "Epoch 110: loss improved from 0.68721 to 0.67964, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6796 - accuracy: 0.7838 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.6765 - accuracy: 0.7866\n",
            "Epoch 111: loss improved from 0.67964 to 0.67932, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.6793 - accuracy: 0.7854 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.6702 - accuracy: 0.7901\n",
            "Epoch 112: loss improved from 0.67932 to 0.67044, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6704 - accuracy: 0.7890 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.6590 - accuracy: 0.7903\n",
            "Epoch 113: loss improved from 0.67044 to 0.65994, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.6599 - accuracy: 0.7898 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.6586 - accuracy: 0.7911\n",
            "Epoch 114: loss improved from 0.65994 to 0.65706, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6571 - accuracy: 0.7910 - lr: 2.0000e-04\n",
            "Epoch 115/150\n",
            "83/87 [===========================>..] - ETA: 0s - loss: 0.6503 - accuracy: 0.7929\n",
            "Epoch 115: loss improved from 0.65706 to 0.65414, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.6541 - accuracy: 0.7916 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.6558 - accuracy: 0.7879\n",
            "Epoch 116: loss did not improve from 0.65414\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 0.6597 - accuracy: 0.7865 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.6591 - accuracy: 0.7930\n",
            "Epoch 117: loss did not improve from 0.65414\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 0.6643 - accuracy: 0.7910 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.7851\n",
            "Epoch 118: loss did not improve from 0.65414\n",
            "\n",
            "Epoch 118: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 0.6674 - accuracy: 0.7851 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.8178\n",
            "Epoch 119: loss improved from 0.65414 to 0.60492, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.6049 - accuracy: 0.8178 - lr: 1.0000e-04\n",
            "Epoch 120/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5807 - accuracy: 0.8337\n",
            "Epoch 120: loss improved from 0.60492 to 0.58201, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5820 - accuracy: 0.8328 - lr: 1.0000e-04\n",
            "Epoch 121/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5702 - accuracy: 0.8390\n",
            "Epoch 121: loss improved from 0.58201 to 0.57271, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5727 - accuracy: 0.8379 - lr: 1.0000e-04\n",
            "Epoch 122/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5670 - accuracy: 0.8419\n",
            "Epoch 122: loss improved from 0.57271 to 0.56970, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5697 - accuracy: 0.8407 - lr: 1.0000e-04\n",
            "Epoch 123/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.8376\n",
            "Epoch 123: loss improved from 0.56970 to 0.56704, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5670 - accuracy: 0.8377 - lr: 1.0000e-04\n",
            "Epoch 124/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.8400\n",
            "Epoch 124: loss improved from 0.56704 to 0.56458, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5646 - accuracy: 0.8400 - lr: 1.0000e-04\n",
            "Epoch 125/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5621 - accuracy: 0.8432\n",
            "Epoch 125: loss improved from 0.56458 to 0.56350, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 17ms/step - loss: 0.5635 - accuracy: 0.8426 - lr: 1.0000e-04\n",
            "Epoch 126/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5632 - accuracy: 0.8410\n",
            "Epoch 126: loss improved from 0.56350 to 0.56041, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5604 - accuracy: 0.8418 - lr: 1.0000e-04\n",
            "Epoch 127/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5593 - accuracy: 0.8438\n",
            "Epoch 127: loss improved from 0.56041 to 0.55900, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5590 - accuracy: 0.8431 - lr: 1.0000e-04\n",
            "Epoch 128/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5567 - accuracy: 0.8456\n",
            "Epoch 128: loss improved from 0.55900 to 0.55592, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5559 - accuracy: 0.8451 - lr: 1.0000e-04\n",
            "Epoch 129/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5532 - accuracy: 0.8438\n",
            "Epoch 129: loss improved from 0.55592 to 0.55539, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.5554 - accuracy: 0.8422 - lr: 1.0000e-04\n",
            "Epoch 130/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5540 - accuracy: 0.8443\n",
            "Epoch 130: loss improved from 0.55539 to 0.55446, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5545 - accuracy: 0.8442 - lr: 1.0000e-04\n",
            "Epoch 131/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.8407\n",
            "Epoch 131: loss improved from 0.55446 to 0.55257, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5526 - accuracy: 0.8407 - lr: 1.0000e-04\n",
            "Epoch 132/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5511 - accuracy: 0.8387\n",
            "Epoch 132: loss improved from 0.55257 to 0.55193, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5519 - accuracy: 0.8388 - lr: 1.0000e-04\n",
            "Epoch 133/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5492 - accuracy: 0.8458\n",
            "Epoch 133: loss improved from 0.55193 to 0.54878, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5488 - accuracy: 0.8447 - lr: 1.0000e-04\n",
            "Epoch 134/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5484 - accuracy: 0.8450\n",
            "Epoch 134: loss improved from 0.54878 to 0.54877, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5488 - accuracy: 0.8451 - lr: 1.0000e-04\n",
            "Epoch 135/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5442 - accuracy: 0.8445\n",
            "Epoch 135: loss improved from 0.54877 to 0.54472, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5447 - accuracy: 0.8444 - lr: 1.0000e-04\n",
            "Epoch 136/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.5463 - accuracy: 0.8402\n",
            "Epoch 136: loss did not improve from 0.54472\n",
            "87/87 [==============================] - 1s 15ms/step - loss: 0.5463 - accuracy: 0.8402 - lr: 1.0000e-04\n",
            "Epoch 137/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.5405 - accuracy: 0.8483\n",
            "Epoch 137: loss improved from 0.54472 to 0.54047, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 20ms/step - loss: 0.5405 - accuracy: 0.8483 - lr: 1.0000e-04\n",
            "Epoch 138/150\n",
            "87/87 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.8451\n",
            "Epoch 138: loss did not improve from 0.54047\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 0.5427 - accuracy: 0.8451 - lr: 1.0000e-04\n",
            "Epoch 139/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5331 - accuracy: 0.8491\n",
            "Epoch 139: loss improved from 0.54047 to 0.53915, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5391 - accuracy: 0.8471 - lr: 1.0000e-04\n",
            "Epoch 140/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5342 - accuracy: 0.8489\n",
            "Epoch 140: loss improved from 0.53915 to 0.53609, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5361 - accuracy: 0.8482 - lr: 1.0000e-04\n",
            "Epoch 141/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5297 - accuracy: 0.8480\n",
            "Epoch 141: loss improved from 0.53609 to 0.53359, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5336 - accuracy: 0.8458 - lr: 1.0000e-04\n",
            "Epoch 142/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5327 - accuracy: 0.8454\n",
            "Epoch 142: loss did not improve from 0.53359\n",
            "87/87 [==============================] - 1s 14ms/step - loss: 0.5355 - accuracy: 0.8440 - lr: 1.0000e-04\n",
            "Epoch 143/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5274 - accuracy: 0.8475\n",
            "Epoch 143: loss improved from 0.53359 to 0.53122, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5312 - accuracy: 0.8462 - lr: 1.0000e-04\n",
            "Epoch 144/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5282 - accuracy: 0.8505\n",
            "Epoch 144: loss improved from 0.53122 to 0.52746, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5275 - accuracy: 0.8503 - lr: 1.0000e-04\n",
            "Epoch 145/150\n",
            "86/87 [============================>.] - ETA: 0s - loss: 0.5258 - accuracy: 0.8510\n",
            "Epoch 145: loss improved from 0.52746 to 0.52546, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.5255 - accuracy: 0.8512 - lr: 1.0000e-04\n",
            "Epoch 146/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.8511\n",
            "Epoch 146: loss improved from 0.52546 to 0.52536, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5254 - accuracy: 0.8494 - lr: 1.0000e-04\n",
            "Epoch 147/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.8489\n",
            "Epoch 147: loss improved from 0.52536 to 0.52374, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.5237 - accuracy: 0.8492 - lr: 1.0000e-04\n",
            "Epoch 148/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5285 - accuracy: 0.8451\n",
            "Epoch 148: loss did not improve from 0.52374\n",
            "87/87 [==============================] - 1s 13ms/step - loss: 0.5257 - accuracy: 0.8449 - lr: 1.0000e-04\n",
            "Epoch 149/150\n",
            "84/87 [===========================>..] - ETA: 0s - loss: 0.5199 - accuracy: 0.8501\n",
            "Epoch 149: loss improved from 0.52374 to 0.52112, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 18ms/step - loss: 0.5211 - accuracy: 0.8498 - lr: 1.0000e-04\n",
            "Epoch 150/150\n",
            "85/87 [============================>.] - ETA: 0s - loss: 0.5152 - accuracy: 0.8526\n",
            "Epoch 150: loss improved from 0.52112 to 0.51660, saving model to nextword1.h5\n",
            "87/87 [==============================] - 2s 19ms/step - loss: 0.5166 - accuracy: 0.8518 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "input_text = input().strip().lower()\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, padding='pre', truncating='pre')\n",
        "print(encoded_text, pad_encoded)\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "  pred_word = tokenizer.index_word[i]\n",
        "  print(\"Next word suggestion:\",pred_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRkFQ5EK_nHo",
        "outputId": "ef0d63f9-b4ab-4973-afd0-db14d5be99d3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he found himself \n",
            "[13, 725, 37] [[725  37]]\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "Next word suggestion: junk\n",
            "Next word suggestion: wild\n",
            "Next word suggestion: accept\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "\n",
        "Modify your auto-complete so that it never suggests names of people or places.\n",
        "Include a code snippet and examples in your response.\n"
      ],
      "metadata": {
        "id": "Pu-GzMi3GM1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8vZcUU3M9HL",
        "outputId": "a53dc250-c1ac-4d1d-c164-4507383170b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import ne_chunk\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "ne_tree = ne_chunk(pos_tag(word_tokenize(data)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5UqsRnLf8mh",
        "outputId": "68fb5757-c93a-40c6-8d79-5c87b2f9443f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import Tree\n",
        "nltk_results = ne_chunk(pos_tag(word_tokenize(data)))\n",
        "for nltk_result in nltk_results:\n",
        "    if type(nltk_result) == Tree:\n",
        "        name = ''\n",
        "        for nltk_result_leaf in nltk_result.leaves():\n",
        "            name += nltk_result_leaf[0] + ' '\n",
        "        print('Type: ', nltk_result.label(), ' Name: ', name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrWGh3OXnVP9",
        "outputId": "b1cdb6cb-0422-455f-f468-bd6bdf6273c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type:  PERSON  Name:  Gregor Samsa \n",
            "Type:  PERSON  Name:  Samsa \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  ORGANIZATION  Name:  Hell \n",
            "Type:  GPE  Name:  Heaven \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Anyway \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Sir \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Please \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  God \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Quick \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Anna \n",
            "Type:  PERSON  Name:  Anna \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Help \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Milk \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  God \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Half \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Charlottenstrasse \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Arms \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Mother \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  French \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Smears \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  ORGANIZATION  Name:  Christmas \n",
            "Type:  PERSON  Name:  Christmas \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Samsa \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Mother \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Father \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Father \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Mr. \n",
            "Type:  PERSON  Name:  Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Gregor \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  ORGANIZATION  Name:  Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  GPE  Name:  Grete \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Cheerio \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  PERSON  Name:  Mr. Samsa \n",
            "Type:  GPE  Name:  Gregor \n",
            "Type:  PERSON  Name:  Grete \n",
            "Type:  PERSON  Name:  Mr. \n",
            "Type:  PERSON  Name:  Grete \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea is to to remove the output from the line above from the data, then retrain it with the LSTM model. The suggestion result won't include name and locations. As one can easily spot some mistakes from the result above, one can manually remove them from the data. "
      ],
      "metadata": {
        "id": "ZDow6WlM0H3A"
      }
    }
  ]
}